---
layout: default
---
## Ting Han
Research scientist<br>
<a href="https://www.aist.go.jp/waterfront/index_en.html" target="_blank"> Artificial Intelligence Research Center(AIST), Tokyo, Japan</a><br>
Email: firstname.lastname@aist.go.jp
### About Me
I’m interested in enabling AIs to understand natural human communications. My PhD research focuses on exploring how natural language informs the interpretation of co-verbal gestures.

I received my PhD from <a href="https://www.uni-bielefeld.de" target="_blank"> Bielefeld University</a>, advised by Prof. Dr. David Schlangen.  I was also a member of the <a href="http://www.cit-ec.de" target="_blank"> Cognitive Interaction Technology Center of Excellence (CITEC)</a> graduate school. Previously I earned a M.S. in computer vision and B.S. in computer science, both from Lanzhou University in China.


### Publications
1. **Enabling Robots to Draw and Tell: Towards Visually Grounded Multimodal Description Generation**.  <u>Ting Han</u> and Sina Zarrieß. *The 2nd Workshop on NLG for HRI (NLG4HRI co-located with INLG2020)*, 15 December 2020. [PDF]()

1. **Mandarinograd: A Chinese Collection of Winograd Schemas**.  Timothee Bernard and <u>Ting Han</u>. *The 12th edition of the Language Resources and Evaluation Conference (LREC2020)*, 13-15 May 2020, Marseille, France. [PDF](files/papers/LREC20.pdf)
1. **Sketch Me if You Can: Towards Generating Detailed Descriptions of Object Shape by Grounding in Images and Drawings**.   <u>Ting Han</u>, and Sina Zarrieß.  *Proceedings of the 12th International Conference on Natural Language Generation (INLG19)*, October 19 -November 1 2019, Tokyo, Japan. [PDF](files/papers/inlg19.pdf)
1. **Bridging the Gap between Robotic Applications andComputational Intelligence - An Overview on Domestic Robotics**.   Junpei Zhong, <u>Ting Han</u>, Ahmad Lotfi, Angelo Cangelosi, and Xiaofeng Liu.  *2019 IEEE Symposium Series on Computational Intelligence (IEE DR)*, December 6-9 2019, Xiamen, China. [PDF](files/papers/semdial18.pdf)
1. **Learning to Describe Multimodally from Parallel Unimodal Data? A Pilot Study on Verbal and Sketched Object Descriptions**.   <u>Ting Han</u>,  Sina Zarrieß, Kazunori Komatani, and David Schlangen.  *Proceedings of the 22nd Workshop on the Semantics and Pragmatics of Dialogue (AixDial)*, November 8-10th 2018, Aix-en-Provence (France) [PDF](files/papers/semdial18.pdf)
1.  **Learning to Interpret and Apply Multimodal Descriptions**. <u>Ting Han</u>. *Bielefeld University*, 2018.
1. **A Corpus of Natural Multimodal Spatial Scene Descriptions**.  <u>Ting Han</u> and David Schlangen. *The 11th edition of the Language Resources and Evaluation Conference (LREC18)*, 7-12 May 2018, Miyazaki, Japan. [pdf](files/papers/lrec18.pdf)  [bib](bib/Hanlrec18.txt)
1. **Placing Objects in Gesture Space: Toward Real-Time Understanding of Spatial Descriptions**. <u>Ting Han</u>, Casey Kennington and David Schlangen.  *The thrity-second AAAI conference on artificial intelligence (AAAI18), Feburary 2-7, New Orleans, Louisiana, USA, 2018*. [pdf](files/papers/aaai18.pdf)   [bib](bib/HanEtalaaai18.txt)
1.  **Draw and Tell: Multimodal Descriptions Outperform Verbal- or Sketch-Only Descriptions in an Image Retrieving Task**.  <u>Ting Han</u> and David Schlangen. *Inproceedings of  the 8th international joint conference of natural language processing (IJCNLP17), Taipei, Taiwan, 2017*.  [pdf](files/papers/sketch_ijcnlp_short.pdf) [PUB](https://pub.uni-bielefeld.de/publication/2913598)
1.  **Natural Language Informs the Interpretation of Iconic Gestures: A Computational Approach**. <u>Ting Han</u>, Julian Hough, and David Schlangen. *Inproceedings of the 8th international joint conference of natural language processing (IJCNLP17), Taipei, Taiwan, 2017*.  [PUB](https://pub.uni-bielefeld.de/publication/2913599)
1. **Temporal Alignment Using the Incremental Unit Framework**. Casey Kenninton,  <u>Ting Han</u>, and David Schlangen, *Proceedings of 19th ACM International Conference on Multimodal Interaction, Glasgow, Scotland, November 13-17th, 2017 (ICMI17).*   [PUB](https://pub.uni-bielefeld.de/publication/2913600)
3. **Grounding Language by Continuous Observation of Instruction Following**.  <u>Ting Han</u> and David Schlangen, *Proceedings of the Annual Meeting of the European Chapter of the Association for Computational Linguistics (EACL), Valencia: 491-496, 2017*. [PUB](https://pub.uni-bielefeld.de/publication/2908812)
4. **Building and Applying Perceptually-Grounded Representations of Multimodal Scene Descriptions**. <u>Ting Han</u>, Casey Kennington, and David Schlangen, *Proceedings of the 19th SemDial Workshop on the Semantics and Pragmatics of Dialogue (goDIAL)* 2015. [PUB](https://pub.uni-bielefeld.de/publication/2758943)
5. **A Corpus of Virtual Pointing Gestures**. <u>Ting Han</u>, Spyros Kousidis, and David Schlangen, *Presented at the REFNET Workshop on Computational and Psychological Models of Reference Comprehension and Production, Edinburgh* 2014. [Poster] [PUB](https://pub.uni-bielefeld.de/publication/2685979)
6. **Towards Automatic Understanding of ‘Virtual Pointing’ in Interaction**.  <u>Ting Han</u>, Spyros Kousidis, and David Schlangen, *Proceedings of the 18th SemDial Workshop on the Semantics and Pragmatics of Dialogue (DialWatt), Posters. Herriot-Watt University; 2014: 188-190*. [poster] [PUB](https://pub.uni-bielefeld.de/publication/2685950)
7. **A Fast Dark Channel Prior-Based Depth Map Approximation Method for Dehazing Single Images**. <u>Ting Han</u> and Yi Wan. *In International Conference on Information Science and Technology (ICIST), 2013, pages 1355–1359.* [PUB](http://ieeexplore.ieee.org/abstract/document/6747789/)
